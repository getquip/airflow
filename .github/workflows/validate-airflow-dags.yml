name: Validate Airflow DAGs

# Trigger this workflow when a PR is opened or updated (commits pushed).
on:
  pull_request:
    types:
      - opened        # Trigger when a new PR is opened
      - synchronize   # Trigger when new commits are added to an open PR
      - reopened      # Trigger when a PR is reopened
      - ready_for_review # Trigger when a PR is marked as ready for review

env:
  AIRFLOW_VERSION: "2.10.2"  # Match the Airflow version in Composer 
  GH_TOKEN: ${{ github.token }}  # GitHub token to access the PR files
  GCS_BUCKET_NAME: ${{ secrets.AIRFLOW_DEV_BUCKET }}
  GCP_PROJECT_ID: ${{ secrets.GCP_DEV_PROJECT_ID }}  
  GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
  
jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout the code to access the files in the PR
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Fetch full history

      # 2. Set up Python environment to run validation
      - name: Set up Python environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'  # Set up Python 3.9 (or your preferred version)

      # 4. Find new or modified scripts in the `dags/` directory
      - name: Find new or modified DAGs
        id: find_dags
        run: |
          # Change to the repository directory
          cd dags
          
          # Fetch base branch to compare with the PR's head
          git fetch origin ${{ github.event.pull_request.base.ref }}
          
          # Get changed files between base and head
          changed_files=$(git diff --name-only --diff-filter=ACMRTUXB origin/${{ github.event.pull_request.base.ref }}...${{ github.event.pull_request.head.sha }} | xargs)

          # Convert the space-separated string into an array
          IFS=' ' read -r -a changed_files_array <<< "$changed_files"

          # Get patters from .airflowignore
          mapfile -t ignore_patterns < .airflowignore

          # Initialize an empty array for new DAG files
          new_dag_files=()

          # Loop through the changed files and filter out the new/modified DAG files
          for file in "${changed_files_array[@]}"; do
            # Check if the file starts with "dags/" and is not the .airflowignore file
            if [[ "$file" == dags/* ]] && [[ "$file" != dags/.airflowignore ]]; then 
              ignore_file=false

              # Check if the file is in the ignore list
              for pattern in "${ignore_patterns[@]}"; do
                if [[ "$file" =~ $pattern ]]; then
                  ignore_file=true
                  break
                fi
              done

              # If no match found, add the file to the new_dag_files array
              if [ "$ignore_file" = false ]; then
                dag_name="${file/dags\//}" # Remove the 'dags/' prefix
                new_dag_files+=("$dag_name")
              fi
            fi
          done

          # Check if there are no new/modified DAG files
          if [ -z "$new_dag_files" ]; then
            echo "No new or modified DAG files. Exiting workflow."
            exit 0  # Exit the workflow if no new DAG files are found
          fi

          echo "New or modified DAG files:"
          echo "${new_dag_files[@]}"
        
          # Save the list of new/modified DAG files as an environment variable
          echo "new_dags=${new_dag_files[*]}" >> $GITHUB_ENV

      # 5. Install the correct version of Apache Airflow
      - name: Install Airflow and other dependencies
        if: env.new_dags != ''  # Only run this step if there are new/modified DAG files
        run: |
          python -m pip install --upgrade pip
          CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-3.9.txt"
          pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
          pip install apache-airflow[virtualenv]
          pip install apache-airflow[cncf.kubernetes]
          pip install -r requirements.txt

      # 6 Authenticate with Google Cloud SDK
      - name: Authenticate with Google Cloud SDK
        if: env.new_dags != ''  # Only run this step if there are new/modified DAG files
        run: |
          echo $GCP_CREDENTIALS_JSON > gcp_key.json
          gcloud auth activate-service-account --key-file=gcp_key.json
          gcloud config set project $GCP_PROJECT_ID

      # 7. Validate if any new DAG scripts are valid
      - name: Validate new DAGs
        if: env.new_dags != ''  # Only run this step if there are new/modified DAG files
        run: |
          airflow db migrate
          # Read the space-separated string into an array
          IFS=' ' read -r -a new_dags <<< "$new_dags"

          # Loop through each DAG file and validate
          for dag_file in "${new_dags[@]}"; do
            echo "Validating DAG: $dag_file"

            # Validate the DAG file
            python -c "from airflow.models import DagBag;from airflow.models import Variable; Variable.set("PROJECT_ID", "quip-dw-raw-dev");Variable.set("GCS_BUCKET", "quip_airflow_dev"); dag_bag = DagBag(include_examples = False); print(list(dag_bag.dags.keys())); assert dag_bag.dags.get('$dag_file'); assert not dag_bag.import_errors, dag_bag.import_errors " || exit 1
          done


from airflow.models import DagBag
from airflow.models import Variable
Variable.set("project_id", "quip-dw-raw-dev")
Variable.set("GCS_BUCKET", "quip_airflow_dev") 
dag_bag = DagBag(include_examples = False)
myDag=dag_bag.process_file('get__recharge.py')